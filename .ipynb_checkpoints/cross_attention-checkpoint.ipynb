{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fc14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim=1024, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        # 64\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.query = nn.Linear(dim, dim)\n",
    "        self.key = nn.Linear(dim, dim)\n",
    "        self.value = nn.Linear(dim, dim)\n",
    "        self.out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        q = self.query(x).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(y).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(y).view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "        \n",
    "        # 压缩序列长度维度\n",
    "        out = out.mean(dim=1)  \n",
    "        \n",
    "        return self.out(out)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
